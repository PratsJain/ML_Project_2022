# ML Project 2022
## Study of Gender and Disability Bias in sentiment Analysis Models  
Machine learning models are thought to be neutral and without any human bias. But many studies show that it is not the case. They are built upon historically generated data, and through human evolution, our society has evolved with inherent biases. So, the data that we use is a reflection of the same. The machine learning systems trained on these datasets result in highlighting inappropriate human biases. We present our study on the same.  

Automated systems such as machine learning models continue to have a major impact on various aspects of human life. These systems have helped us in many ways and have also promoted equality and fairness by alleviating human biases from several activities that might have induced partiality. For example, we cannot manipulate machine learning systems by offering them bribes to influence decisions. Another example would be the automated systems that are used to predict the eligibility of a person for a loan regardless of their skin colour.  

Although these automated systems have helped in eradicating human biases in some situations, they also tend to depict human biases in most of the systems. It is seen that as these models become more human-like in their predictions, they also tend to incorporate biases that have been present in the society. For example, resume sorting systems may incorporate a gender bias while looking for programmers. This can be due to various reasons like training data, other corpora, lexicons, and word embeddings used to build the machine learning model.  

The models that have induced implicit biases would result in catalyzing already existing inequalities in the human society. Hence, there have been various studies on identifying implicit biases in NLP techniques, mainly sentiment analysis models. Some notable studies include the detection of race and gender bias, the study of age-related bias, and identification of bias against people with disabilities.  

In this project, we train five different sentiment analysis models on two different datasets and then check if the models have statistically significant bias. We are using IMDB reviews dataset and jigsaw toxic comment classification dataset to train our models. The models include TextBlob, Logistic Regression, SVM, Naive Bayes and LSTM Networks. We test the model trained on IMDB reviews dataset for gender bias using the Equity Evaluation Corpus(EEC). We test the model trained on jigsaw toxic comment classification dataset for bias against disabled people using the Bias Identification Test in Sentiments (BITS Corpus). We find that all the models except TextBlob and LSTM show significant statistical gender bias. We also report that only Naive Bayes had statistically significant disability bias. Furthermore, LSTM performs best on the training as well as on the test sets on both datasets.   

